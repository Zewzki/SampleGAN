{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResearchProject.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TCXqnRXNWXPV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"c673c0c6-0a0a-4d7c-cc3b-c47b58fb4317","executionInfo":{"status":"ok","timestamp":1557706717063,"user_tz":300,"elapsed":11269,"user":{"displayName":"Michael Wisnewski","photoUrl":"","userId":"08288731010122458134"}}},"source":["!pip install wave\n","!pip install pydub\n","!pip install scipy\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.utils import plot_model\n","import time\n","import numpy as np\n","import wave\n","import pydub\n","from pydub import AudioSegment\n","from pydub.utils import make_chunks\n","from pydub.playback import play\n","import os\n","from os import path\n","from scipy.io.wavfile import read\n","from scipy.io.wavfile import write"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting wave\n","  Downloading https://files.pythonhosted.org/packages/df/33/5a06e0c47a147b2683876ba7c576fad13e92b0b16755eb431e56c341e0cf/Wave-0.0.2.tar.gz\n","Building wheels for collected packages: wave\n","  Building wheel for wave (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/8c/2e/ad/d96151afb1fdccf126346b26eabb91fec3c5ce5cbee7287fbf\n","Successfully built wave\n","Installing collected packages: wave\n","Successfully installed wave-0.0.2\n","Collecting pydub\n","  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n","Installing collected packages: pydub\n","Successfully installed pydub-0.23.1\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.3)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BBJiJroNqTsI","colab_type":"code","colab":{}},"source":["# load in data\n","def loadSongs():\n","  \n","  pathContent = os.listdir()\n","  mp3List = []\n","  wavList = []\n","  \n","  for s in pathContent:\n","    if(s.find('.mp3') >= 0):\n","      mp3List.append(s)\n","    if(s.find('.wav') >= 0):\n","      wavList.append(s)\n","  \n","  fullSongs = []\n","  \n","  for path in mp3List:\n","    dest = path.replace('.mp3', '.wav')\n","    sound = AudioSegment.from_mp3(path)\n","    \n","    # conver file to mono\n","    sound = sound.set_channels(1)\n","    sound.export(dest, format = 'wav')\n","    \n","    fullSongs.append(dest)\n","    \n","  for path in wavList:\n","    fullSongs.append(path)\n","  \n","  return fullSongs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRLD7GBN3QkQ","colab_type":"code","colab":{}},"source":["# stride = 1 for stateless, stride = lookback for stateful\n","def createDataset(songList, lookback, predict, stride, validationSplit):\n","  \n","  s = 'seg'\n","  e = '.wav'\n","  \n","  x_train = []\n","  y_train = []\n","  \n","  x_test = []\n","  y_test = []\n","  \n","  totalSegs = 0\n","  validationSplit = validationSplit * 100\n","  \n","  for path in songList:\n","    \n","    wav = read(path)\n","    arr = wav[1]\n","    \n","    #wav = AudioSegment.from_wav(path)\n","    #arr = wav.get_array_of_samples()\n","    #arr = np.array(arr)\n","    \n","    # TODO lookback + 1 or not?\n","    for i in range(lookback, len(arr) - predict, stride):\n","      hist = arr[i - lookback : i]\n","      future = arr[i : i + predict]\n","      if(totalSegs % validationSplit == 0):\n","        x_test.append(hist)\n","        y_test.append(future)\n","      else:\n","        x_train.append(hist)\n","        y_train.append(future)\n","      \n","      totalSegs = totalSegs + 1\n","      \n","      \n","  print('Dataset created')\n","  \n","  return x_train, y_train, x_test, y_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HXq_UW8rzcn","colab_type":"code","colab":{}},"source":["def createEncoderDataset(songList, size, stride):\n","  \n","  splices = []\n","  \n","  for path in songList:\n","    \n","    wav = read(path)\n","    arr = wav[1]\n","    \n","    for i in range(size, len(arr), stride):\n","      splice = arr[i - size : i]\n","      splices.append(splice)\n","      \n","  return splices"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"evZmXQzHSrHQ","colab_type":"code","colab":{}},"source":["def lstmAutoencoder(inputSize, outputSize):\n","  \n","  model = keras.models.Sequential()\n","  \n","  # encoder\n","  \n","  model.add(keras.layers.Conv1D(32, 3, activation = 'relu', padding = 'same', input_shape = (inputSize, 1)))\n","  model.add(keras.layers.Conv1D(32, 3, activation = 'relu', padding = 'same'))\n","  model.add(keras.layers.MaxPooling1D(pool_size = 2))\n","  model.add(keras.layers.Conv1D(64, 3, activation = 'relu', padding = 'same'))\n","  model.add(keras.layers.Conv1D(64, 3, activation = 'relu', padding = 'same'))\n","  model.add(keras.layers.MaxPooling1D(pool_size = 2))\n","  model.add(keras.layers.Conv1D(128, 3, activation = 'relu', padding = 'same'))\n","  model.add(keras.layers.Conv1D(128, 3, activation = 'relu', padding = 'same'))\n","  model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n","  plot_model(model, show_shapes = True, to_file='autoencoder.png')\n","  \n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdtyj4kFKnxM","colab_type":"code","colab":{}},"source":["def lstmEncoder(inputSize, outputSize):\n","  model = keras.models.Sequential()\n","  model.add(keras.layers.CuDNNLSTM(64, return_sequences = True, input_shape = (inputSize, 1)))\n","  model.add(keras.layers.TimeDistributed(keras.layers.Dense(outputSize)))\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9MesCVjNj5T","colab_type":"code","colab":{}},"source":["def lstmDecoder(inputSize, outputSize):\n","  model = keras.models.Sequential()\n","  model.add(keras.layers.Dense(outputSize))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0GTSOGQapIA","colab_type":"code","colab":{}},"source":["def lstmGen(lookback, predict):\n","  model = keras.models.Sequential()\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = True, input_shape = (lookback, predict)))\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = False))\n","  model.add(keras.layers.Dense(predict, activation = 'tanh'))\n","  model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])\n","  plot_model(model, show_shapes = True, to_file='generator.png')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnSN7Kxig7FU","colab_type":"code","colab":{}},"source":["def lstmDisc(lookback, predict):\n","  model = keras.models.Sequential()\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = True, input_shape = (lookback, predict)))\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = False))\n","  model.add(keras.layers.Dense(1))\n","  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","  plot_model(model, show_shapes = True, to_file='discriminator.png')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2IKvRxdyAI2","colab_type":"code","colab":{}},"source":["def lstm(lookback, predict):\n","  model = keras.models.Sequential()\n","  model.add(keras.layers.Conv1D(128, 3, input_shape = [lookback, predict]))\n","  model.add(keras.layers.MaxPooling1D())\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = True))\n","  model.add(keras.layers.Conv1D(64, 5))\n","  model.add(keras.layers.MaxPooling1D())\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = True))\n","  model.add(keras.layers.Conv1D(32, 5))\n","  model.add(keras.layers.MaxPooling1D())\n","  model.add(keras.layers.CuDNNLSTM(128, return_sequences = False))\n","  model.add(keras.layers.Dense(predict, 'tanh'))\n","  model.summary()\n","  model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n","  plot_model(model, show_shapes = True, to_file='lstm.png')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-u8QCOMYqRcA","colab_type":"code","colab":{}},"source":["def trainGan(epochs):\n","  for epoch in range(epochs):\n","    \n","    # first, generate images\n","    idx = np.random.randint(0, rpn_x_train.shape[0], batch_size)\n","    true = np.array(rpn_x_train[idx])\n","    print(true.shape)\n","    true = np.expand_dims(true, axis = 3)\n","    noise = np.random.normal(0, 1, (batch_size, lookback, predict))\n","    fake = genSequences(gen, batch_size, lookback, noise, printDiag = False)\n","    \n","    fake = np.array(fake)\n","    print(fake.shape)\n","    \n","    validLabel = np.ones((batch_size, 1))\n","    fakeLabel = np.zeros((batch_size, 1))\n","    \n","    #print(validLabel.shape)\n","    #print(fakeLabel.shape)\n","    #print(true.shape)\n","    #print(fake.shape)\n","    \n","    # train discriminator\n","    \n","    d_loss_real = disc.train_on_batch(true, validLabel)\n","    d_loss_fake = disc.train_on_batch(fake, fakeLabel)\n","    d_loss = .5 * (np.add(d_loss_real, d_loss_fake))\n","    \n","    # train generator\n","    noise_tr = np.random.normal(0, 1, (batch_size, lookback))\n","    noise_tr = np.expand_dims(noise_tr, axis = 3)\n","    \n","    disc.trainable = False\n","    g_loss = gen.train_on_batch(noise_tr, validLabel)\n","    \n","    disc.trainable = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaZl_c8RqiRe","colab_type":"code","colab":{}},"source":["def trainAutoencoder(model, x_train, epochs, validationSplit):\n","  model.fit(x_train, x_train, epochs = epochs, validation_split = validationSplit)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpc7CsS6sP0d","colab_type":"code","colab":{}},"source":["def genSequences(gen, numOfSequences, size, noise, printDiag = False):\n","  \n","  sequences = []\n","  \n","  printIndex = size / 10\n","  \n","  for i in range(numOfSequences):\n","    \n","    currSequence = []\n","    currNoise = np.array(noise)\n","    \n","    if(printDiag):\n","      print(\"Generating Sequence \", i)\n","      start = time.time()\n","    \n","    for j in range(size):\n","    \n","      prediction = gen.predict(np.array(currNoise))\n","      \n","      currSequence.append(prediction[0])\n","      currNoise = np.roll(currNoise, -1)\n","      currNoise[currNoise.shape[0] - 1] = prediction[0]\n","      \n","      if(j % printIndex == 0 and printDiag):\n","        diff = time.time() - start\n","        start = time.time()\n","        print(j, \" of \", size, \" in \", diff)\n","    \n","    sequences.append(currSequence)\n","    \n","  return sequences"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ydtiDDsgOxm","colab_type":"code","colab":{}},"source":["def arrToWav(dir, samples):\n","  \n","  samples = np.array(samples)\n","  \n","  write(dir, 44100, samples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hF3P3r45BQ8M","colab_type":"code","colab":{}},"source":["def saveModel(model, name):\n","  \n","  model_json = model.to_json()\n","  jsonName = name + '.json'\n","  h5Name = name + '.h5'\n","  with open(jsonName, \"w\") as json_file:\n","    json_file.write(model_json)\n","    \n","  # serialize weights to HDF5\n","  model.save_weights(h5Name)\n","  print(\"Saved model to disk\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUPlFSnRKVMd","colab_type":"code","outputId":"0f9a8072-7199-44bc-ca0d-ef9779acf2c6","executionInfo":{"status":"ok","timestamp":1556827232225,"user_tz":300,"elapsed":16176,"user":{"displayName":"Michael Wisnewski","photoUrl":"","userId":"08288731010122458134"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["#segNumber = loadSongs()\n","#print(segNumber)\n","\n","#lookback = 44100\n","lookback = 10\n","predict = 1\n","validationSplit = .2\n","batch_size = 3\n","normalizer = 32768\n","spliceSize = lookback * 2\n","\n","songList = loadSongs()\n","x_train, y_train, x_test, y_test = createDataset(songList, lookback, predict, lookback, validationSplit)\n","encoderSplices = createEncoderDataset(songList, spliceSize, spliceSize // 2)\n","\n","rpn_x_train = np.array(x_train)\n","rpn_y_train = np.array(y_train)\n","\n","rpn_x_test = np.array(x_test)\n","rpn_y_test = np.array(y_test)\n","\n","rpn_splices = np.array(encoderSplices)\n","\n","#normalize values; wav is 16 bit signed, so values are between + and - 2^15, so divide by 2^15\n","rpn_x_train = rpn_x_train / normalizer\n","rpn_y_train = rpn_y_train / normalizer\n","\n","rpn_x_test = rpn_x_test / normalizer\n","rpn_y_test = rpn_y_test / normalizer\n","\n","rpn_splices = rpn_splices / normalizer\n","\n","print(rpn_x_train.shape)\n","print(rpn_y_train.shape)\n","\n","print(rpn_x_test.shape)\n","print(rpn_y_test.shape)\n","\n","print(rpn_splices.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset created\n","(8378, 10)\n","(8378, 1)\n","(441, 10)\n","(441, 1)\n","(8818, 20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3vB_LajFhA43","colab_type":"code","colab":{}},"source":["gen = lstmGen(lookback, predict)\n","disc = lstmDisc(lookback, predict)\n","trainGan(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_X-SnPmPPWFW","colab_type":"code","colab":{}},"source":["saveModel(gen, 'gen')\n","saveModel(disc, 'disc')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"haY195RDZC4-","colab_type":"code","colab":{}},"source":["noise = np.random.rand(lookback, 1)\n","testSeq = genSequences(gen, 1, 44100, np.expand_dims(noise, axis = 3), printDiag = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TW6swz_kyzsw","colab_type":"code","colab":{}},"source":["print(rpn_x_train.shape)\n","print(rpn_y_train.shape)\n","\n","x_sub = np.expand_dims(rpn_x_train, axis = 3)\n","\n","model = lstmGen(lookback, predict)\n","model.fit(x_sub, rpn_y_train, epochs = 100, validation_split = 0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6AIG5l4tj4Z","colab_type":"code","colab":{}},"source":["numOfSequences = 1;\n","size = 4410\n","\n","model.summary()\n","noise = rpn_x_train[2]\n","noise = np.expand_dims(noise, axis = 1)\n","noise = np.expand_dims(noise, axis = 0)\n","print(noise.shape)\n","gens = genSequences(model, numOfSequences, size, noise, printDiag = True)\n","#gens = gens * 32768\n","#longSeq = genSequences(model, numOfSequences, size * 6, noise)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"egKwXmL3BuQL","colab_type":"code","outputId":"acd07aef-d961-40e9-d81d-defbbafe51d3","executionInfo":{"status":"ok","timestamp":1556720453743,"user_tz":300,"elapsed":10,"user":{"displayName":"Michael Wisnewski","photoUrl":"","userId":"08288731010122458134"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["saveModel(model, 'lstm')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saved model to disk\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rDWCfjMJp8bO","colab_type":"code","colab":{}},"source":["squeezed = np.squeeze(gens)\n","arrToWav('test2.wav', squeezed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcElJDBVBRKr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"1fb28991-bd70-48a7-d46e-ae1ce22bae86","executionInfo":{"status":"ok","timestamp":1557706720102,"user_tz":300,"elapsed":779,"user":{"displayName":"Michael Wisnewski","photoUrl":"","userId":"08288731010122458134"}}},"source":["ae = lstmAutoencoder(1000, 500)\n","#trainAutoencoder(ae, np.expand_dims(rpn_x_train, axis = 3), 10, .2)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]}]}